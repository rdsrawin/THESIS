{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3dfdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag_pipeline.py\n",
    "\"\"\"\n",
    "End‑to‑end Retrieval‑Augmented Generation (RAG) pipeline\n",
    "======================================================\n",
    "This single script can be run top‑to‑bottom or imported as a module.  It\n",
    "covers the **full stack** you need to build a local proof‑of‑concept QA\n",
    "system:\n",
    "\n",
    "1. **Data ingestion** – reads the pre‑cleaned paragraph file (combo_long.pkl)\n",
    "2. **Chunking**        – token‑bounded with overlap\n",
    "3. **Dual retrieval**  – BM25 (sparse) + MiniLM (dense) hybrid\n",
    "4. **Vector store**    – FAISS index persisted to disk\n",
    "5. **Lite evaluator**  – recall@k and MRR on a YAML gold file\n",
    "6. **FastAPI server**  – /search endpoint returns passages & (optional) LLM answer\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "```bash\n",
    "pip install pandas numpy nltk faiss-cpu             \\\n",
    "            sentence-transformers rank-bm25         \\\n",
    "            pyyaml fastapi uvicorn[standard]         \\\n",
    "            tiktoken openai                         # optional for LLM step\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1619b1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (2.2.5)\n",
      "Requirement already satisfied: nltk in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (3.9.1)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp310-cp310-win_amd64.whl (15.0 MB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Collecting rank-bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (6.0.2)\n",
      "Collecting fastapi\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Collecting uvicorn[standard]\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-win_amd64.whl (894 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.77.0-py3-none-any.whl (662 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from sentence-transformers) (2.7.0+cpu)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.2-cp310-cp310-win_amd64.whl (41.2 MB)\n",
      "Collecting starlette<0.47.0,>=0.40.0\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Collecting python-dotenv>=0.13\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Collecting httptools>=0.6.3\n",
      "  Downloading httptools-0.6.4-cp310-cp310-win_amd64.whl (88 kB)\n",
      "Collecting watchfiles>=0.13\n",
      "  Downloading watchfiles-1.0.5-cp310-cp310-win_amd64.whl (291 kB)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from uvicorn[standard]) (0.4.6)\n",
      "Collecting websockets>=10.4\n",
      "  Downloading websockets-15.0.1-cp310-cp310-win_amd64.whl (176 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting sniffio\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Downloading jiter-0.9.0-cp310-cp310-win_amd64.whl (208 kB)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Collecting anyio<5,>=3.5.0\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\desktop\\feb research\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: sniffio, h11, typing-inspection, threadpoolctl, scipy, pydantic-core, httpcore, anyio, annotated-types, websockets, watchfiles, uvicorn, starlette, scikit-learn, python-dotenv, pydantic, jiter, httpx, httptools, distro, tiktoken, sentence-transformers, rank-bm25, openai, fastapi, faiss-cpu\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.9.0 distro-1.9.0 faiss-cpu-1.11.0 fastapi-0.115.12 h11-0.16.0 httpcore-1.0.9 httptools-0.6.4 httpx-0.28.1 jiter-0.9.0 openai-1.77.0 pydantic-2.11.4 pydantic-core-2.33.2 python-dotenv-1.1.0 rank-bm25-0.2.2 scikit-learn-1.6.1 scipy-1.15.2 sentence-transformers-4.1.0 sniffio-1.3.1 starlette-0.46.2 threadpoolctl-3.6.0 tiktoken-0.9.0 typing-inspection-0.4.0 uvicorn-0.34.2 watchfiles-1.0.5 websockets-15.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\hp\\Desktop\\feb research\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy nltk faiss-cpu sentence-transformers rank-bm25 pyyaml fastapi uvicorn[standard] tiktoken openai  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd98ec3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Desktop\\feb research\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, re, json, unicodedata, functools, textwrap\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "import tiktoken  # for accurate token counts per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6227fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ CONFIG =========================================================\n",
    "VENV_ROOT      = Path(os.environ.get(\"VENV_ROOT\", Path('venv').resolve().parent))\n",
    "PARA_PKL       = VENV_ROOT / \"combo_long.pkl\"\n",
    "FAISS_PATH     = VENV_ROOT / \"para_index.faiss\"\n",
    "META_PATH      = VENV_ROOT / \"para_meta.json\"\n",
    "BM25_PATH      = VENV_ROOT / \"bm25.npy\"\n",
    "LLM_MODEL_NAME = os.environ.get(\"LLM_MODEL\", \"gpt-4o-mini\")  # if using OpenAI\n",
    "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CHUNK_TOKEN    = 350\n",
    "OVERLAP_TOKEN  = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93080526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_nltk():\n",
    "    \"\"\"Download punkt tokenizer if not present.\"\"\"\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/punkt\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09be0376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LOAD & PREP DATA\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def load_paragraphs(path: Path = PARA_PKL) -> pd.DataFrame:\n",
    "    \"\"\"Load the pickled paragraph table created earlier.\"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Expected paragraph pickle at {path}\")\n",
    "    return pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77d489cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CHUNKING UTILITIES\n",
    "# -----------------------------------------------------------------------------\n",
    "_token_cache = {}\n",
    "\n",
    "def _get_encoder(model: str):\n",
    "    if model not in _token_cache:\n",
    "        _token_cache[model] = tiktoken.encoding_for_model(model)\n",
    "    return _token_cache[model]\n",
    "\n",
    "def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
    "    enc = _get_encoder(model)\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def split_text(text: str, max_tokens: int = CHUNK_TOKEN, overlap: int = OVERLAP_TOKEN) -> List[str]:\n",
    "    \"\"\"Split text into chunks with word overlap; uses tiktoken lengths.\"\"\"\n",
    "    words = text.split()\n",
    "    out, start = [], 0\n",
    "    while start < len(words):\n",
    "        end = start + max_tokens\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        out.append(chunk)\n",
    "        start += max_tokens - overlap\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85bec2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. BUILD RETRIEVERS (BM25 + FAISS)\n",
    "# -----------------------------------------------------------------------------\n",
    "class HybridRetriever:\n",
    "    \"\"\"Combines sparse BM25 and dense MiniLM scores.\"\"\"\n",
    "    def __init__(self, emb_model: str = EMB_MODEL_NAME):\n",
    "        self.model   = SentenceTransformer(emb_model)\n",
    "        self.bm25    = None\n",
    "        self.faiss   = None\n",
    "        self.meta    = []\n",
    "            # ── index construction ─────────────────────────────────────────────\n",
    "    def build(self, df: pd.DataFrame, force_rebuild=False):\n",
    "        if FAISS_PATH.exists() and META_PATH.exists() and not force_rebuild:\n",
    "            self._load_index()\n",
    "            return self\n",
    "\n",
    "        ensure_nltk()\n",
    "        all_chunks, meta = [], []\n",
    "        for _, row in df.iterrows():\n",
    "            for chunk in split_text(row[\"Para_list\"]):\n",
    "                cleaned = unicodedata.normalize(\"NFKD\", chunk)\n",
    "                all_chunks.append(cleaned)\n",
    "                meta.append({\"doi\": row.DOI, \"para_id\": int(row.Para_id)})\n",
    "        # BM25\n",
    "        tokenized = [word_tokenize(c.lower()) for c in all_chunks]\n",
    "        self.bm25  = BM25Okapi(tokenized)\n",
    "        np.save(BM25_PATH, tokenized, allow_pickle=True)\n",
    "        # Dense\n",
    "        emb = self.model.encode(all_chunks, batch_size=64, show_progress_bar=True, convert_to_numpy=True).astype(\"float32\")\n",
    "        index = faiss.IndexFlatIP(emb.shape[1])\n",
    "        index.add(emb)\n",
    "        faiss.write_index(index, FAISS_PATH)\n",
    "        # meta\n",
    "        META_PATH.write_text(json.dumps(meta))\n",
    "        self.faiss, self.meta = index, meta\n",
    "        return self\n",
    "\n",
    "    def _load_index(self):\n",
    "        self.faiss = faiss.read_index(str(FAISS_PATH))\n",
    "        self.meta  = json.loads(META_PATH.read_text())\n",
    "        tokenized  = np.load(BM25_PATH, allow_pickle=True)\n",
    "        self.bm25  = BM25Okapi(tokenized)\n",
    "            # ── querying ──────────────────────────────────────────────────────\n",
    "    def search(self, query: str, top_k: int = 6, bm25_weight: float = 0.4):\n",
    "        q_emb = self.model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "        D_dense, I_dense = self.faiss.search(q_emb, top_k)\n",
    "        dense_scores = {int(i): float(s) for i, s in zip(I_dense[0], D_dense[0])}\n",
    "\n",
    "        token_q = word_tokenize(query.lower())\n",
    "        sparse_scores = self.bm25.get_scores(token_q)\n",
    "        # linear combination\n",
    "        hybrid = {}\n",
    "        for idx, dscore in dense_scores.items():\n",
    "            hybrid[idx] = (1 - bm25_weight) * dscore + bm25_weight * sparse_scores[idx]\n",
    "        # fill extras from BM25 if fewer than k\n",
    "        bm25_ranks = np.argsort(-sparse_scores)\n",
    "        for idx in bm25_ranks:\n",
    "            if idx not in hybrid and len(hybrid) < top_k:\n",
    "                hybrid[int(idx)] = sparse_scores[idx]\n",
    "        # return sorted\n",
    "        best = sorted(hybrid.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        out  = [{**self.meta[idx], \"score\": score} for idx, score in best]\n",
    "        return out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
